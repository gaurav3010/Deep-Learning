{"cells":[{"metadata":{"id":"PeozigfFQ850","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"import os\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torchvision\nfrom torch.utils.data import DataLoader, Dataset, random_split\nimport torchvision.transforms as transforms\n\n#For converting the dataset to torchvision dataset format\nclass VowelConsonantDataset(Dataset):\n    def __init__(self, file_path,train=True,transform=None):\n        self.transform = transform\n        self.file_path=file_path\n        self.train=train\n        self.file_names=[file for _,_,files in os.walk(self.file_path) for file in files]\n        self.len = len(self.file_names)\n        if self.train:\n            self.classes_mapping=self.get_classes()\n    def __len__(self):\n        return len(self.file_names)\n    \n    def __getitem__(self, index):\n        file_name=self.file_names[index]\n        image_data=self.pil_loader(self.file_path+\"/\"+file_name)\n        if self.transform:\n            image_data = self.transform(image_data)\n        if self.train:\n            file_name_splitted=file_name.split(\"_\")\n            Y1 = self.classes_mapping[file_name_splitted[0]]\n            Y2 = self.classes_mapping[file_name_splitted[1]]\n            z1,z2=torch.zeros(10),torch.zeros(10)\n            z1[Y1-10],z2[Y2]=1,1\n            label=torch.stack([z1,z2])\n\n            return image_data, label\n\n        else:\n            return image_data, file_name\n          \n    def pil_loader(self,path):\n        with open(path, 'rb') as f:\n            img = Image.open(f)\n            return img.convert('RGB')\n\n      \n    def get_classes(self):\n        classes=[]\n        for name in self.file_names:\n            name_splitted=name.split(\"_\")\n            classes.extend([name_splitted[0],name_splitted[1]])\n        classes=list(set(classes))\n        classes_mapping={}\n        for i,cl in enumerate(sorted(classes)):\n            classes_mapping[cl]=i\n        return classes_mapping\n    ","execution_count":null,"outputs":[]},{"metadata":{"id":"rJJ_MrEVQ_eX","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport torchvision\nimport matplotlib.pyplot as plt\nfrom torchvision import datasets\n\nimport torchvision.transforms as transforms\n\nimport numpy as np\nimport pandas as pd\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","execution_count":null,"outputs":[]},{"metadata":{"id":"S-BX7SIgS_bU","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.ToTensor()])","execution_count":null,"outputs":[]},{"metadata":{"id":"X2cOje6hS_yA","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"full_data = VowelConsonantDataset(\"../input/train/train\",train=True,transform=transform)\ntrain_size = int(0.9 * len(full_data))\ntest_size = len(full_data) - train_size\n\nprint(train_size)\nprint(test_size)\n\ntraindata, validationdata = random_split(full_data, [train_size, test_size])\n\ntrainloader = torch.utils.data.DataLoader(traindata, batch_size=1, shuffle=True)\nvalidationloader = torch.utils.data.DataLoader(validationdata, batch_size=60, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"Uzv8q9j63A_L","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"testdata = VowelConsonantDataset(\"../input/test/test\",train=False,transform=transform)\ntestloader = torch.utils.data.DataLoader(testdata, batch_size=60,shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_classes = 20","execution_count":null,"outputs":[]},{"metadata":{"id":"l0HVEsRCBvjb","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"dataiter = iter(trainloader)\nimages, labels = dataiter.next()\n\nprint(images.shape)\n\nprint(images[0].shape)\nprint(labels[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torchvision import models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vgg = models.vgg16_bn(pretrained = True).to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(vgg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# change out features in in_built vgg model (1000 -> 10)\nfinal_in_features = vgg.classifier[6].in_features\nmod_classifier = list(vgg.classifier.children())[:-1]\nmod_classifier.extend([nn.Linear(final_in_features, num_classes)])\nprint(mod_classifier)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vgg.classifier = nn.Sequential(*mod_classifier)\nprint(vgg.classifier)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vgg = vgg.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluation(dataloader, model):\n    print(dataloader)\n    #total, correct = 0, 0\n    #for data in dataloader:\n    inputs, labels = dataloader.next()\n    print('inside')\n    inputs, labels = inputs.to(device), labels.to(device)\n    outputs = model(inputs)\n    print('Outputs : ',outputs)\n    out1, pred1 = torch.max(outputs[0].data, 1)\n    out2, pred2 = torch.max(outputs[1].data, 1)\n    print('Out1 : ',out1)\n    print('pred1 : ',pred1)\n    print('Out2 : ',out2)\n    print('pred2 : ',pred2)\n    print(labels)\n    print('----------------------------')\n    print('--------------------------------')\n        #total += labels.size(0)\n        #correct += (pred == labels).sum().item()\n        #print(pred==labels)\n      #return 100 * correct / total","execution_count":102,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataiter = iter(trainloader)\nevaluation(dataiter, vgg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MyModel(nn.Module):\n  def __init__(self):\n    super(MyModel, self).__init__()\n    self.cnn_model = nn.Sequential(\n                     nn.Conv2d(3, 6, 5),               # (N, 3, 64, 64) -> (N, 6, 60, 60)\n                     nn.Tanh(),\n                     nn.AvgPool2d(2, stride=2),        # (N, 6, 60, 60) -> (N, 6, 30, 30)\n                     nn.Conv2d(6, 16, 5),              # (N, 6, 30, 30) -> (N, 16, 26, 26)\n                     nn.Tanh(),\n                     nn.AvgPool2d(2, stride=2)         # (N, 16, 26, 26) -> (N, 16, 13, 13)\n    )\n    self.fc_model =  nn.Sequential(\n                     nn.Linear(2704, 120),              # (N, 2704) -> (N, 120)\n                     nn.Tanh(),\n                     nn.Linear(120, 84),               # (N, 120) -> (N, 84)\n                     nn.Tanh(),\n                     nn.Linear(84, 10)                 # (N, 84) -> (N, 10)\n    )\n    \n  def forward(self, x):\n    #print(x.shape)\n    x = self.cnn_model(x)\n    print(x.shape)\n    x = x.view(x.size(0), -1)\n    print(x.shape)\n    y = self.fc_model(x)\n    x = self.fc_model(x)\n    #print(x.shape)\n    return x, y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = MyModel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = model.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataiter = iter(trainloader)\nevaluation(dataiter, model)\n#print(dataiter)","execution_count":103,"outputs":[{"output_type":"stream","text":"<torch.utils.data.dataloader._DataLoaderIter object at 0x7f73fa644390>\ninside\ntorch.Size([1, 16, 13, 13])\ntorch.Size([1, 2704])\nOutputs :  (tensor([[ 0.0256,  0.0493,  0.1078, -0.0389, -0.0663,  0.0416,  0.0684,  0.0676,\n         -0.0163,  0.0948]], device='cuda:0', grad_fn=<AddmmBackward>), tensor([[ 0.0256,  0.0493,  0.1078, -0.0389, -0.0663,  0.0416,  0.0684,  0.0676,\n         -0.0163,  0.0948]], device='cuda:0', grad_fn=<AddmmBackward>))\nOut1 :  tensor([0.1078], device='cuda:0')\npred1 :  tensor([2], device='cuda:0')\nOut2 :  tensor([0.1078], device='cuda:0')\npred2 :  tensor([2], device='cuda:0')\ntensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]]], device='cuda:0')\n----------------------------\n--------------------------------\n","name":"stdout"}]}],"metadata":{"colab":{"name":"PyTorch Data Loader Code for Vowel Consonant Classification.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":1}